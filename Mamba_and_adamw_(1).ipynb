{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1xo9ixMJ6kK",
        "outputId": "e7fa0b2c-5087-4b1d-cffa-fffcfa5fc199"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'KC4.0_MultilingualNMT_retrain'...\n",
            "remote: Enumerating objects: 318, done.\u001b[K\n",
            "remote: Counting objects: 100% (126/126), done.\u001b[K\n",
            "remote: Compressing objects: 100% (78/78), done.\u001b[K\n",
            "remote: Total 318 (delta 56), reused 69 (delta 46), pack-reused 192 (from 2)\u001b[K\n",
            "Receiving objects: 100% (318/318), 66.07 MiB | 11.80 MiB/s, done.\n",
            "Resolving deltas: 100% (92/92), done.\n",
            "Updating files: 100% (182/182), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/dtna2004/KC4.0_MultilingualNMT_retrain.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gkduJg3sDsMD",
        "outputId": "4e2d49d1-4d92-42d0-8439-1a8989b45179"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Requirements**"
      ],
      "metadata": {
        "id": "VGsaAzi-O5U3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch -q"
      ],
      "metadata": {
        "collapsed": true,
        "id": "0C9KVcsVOFG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchtext==0.6.0 -q"
      ],
      "metadata": {
        "collapsed": true,
        "id": "hI2uihGNPnjC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d28fee9a-6c2d-4de1-dc58-b68c41340158"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m84.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m68.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m87.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk pyvi spacy dill -q"
      ],
      "metadata": {
        "collapsed": true,
        "id": "PKbNzmRIPrD1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a652ffd4-49c6-43e4-f01d-a17df0b4d312"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyYAML==6.0.1 -q"
      ],
      "metadata": {
        "collapsed": true,
        "id": "O2M7odgAQA-R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3eee69a8-280e-4e34-8146-2a09889480ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/757.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m368.6/757.7 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m757.7/757.7 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chọn tokenizer**"
      ],
      "metadata": {
        "id": "zidkgWvtQ-9i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "!pip list | grep sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07-DntocQYhz",
        "outputId": "72f0a623-2547-4039-e42d-6148f0d59bab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentencepiece                         0.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train Tokenize**"
      ],
      "metadata": {
        "id": "z7ZAsWzHSERX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "spm.SentencePieceTrainer.train(\n",
        "    input=\"/content/KC4.0_MultilingualNMT_retrain/data/iwslt_en_vi/train2023.vi,/content/KC4.0_MultilingualNMT_retrain/data/iwslt_en_vi/train2023.lo\",\n",
        "    model_prefix=\"vi_lo_bpe\",\n",
        "    vocab_size=12000,\n",
        "    character_coverage=1.0,\n",
        "    model_type=\"bpe\"\n",
        ")"
      ],
      "metadata": {
        "id": "ozw1syXfQ8v4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "nhét vi_lo_bpe.model và .vocab vào repo cho tiện"
      ],
      "metadata": {
        "id": "BmBawuBiSgOk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!find /content -name \"vi_lo_bpe.*\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cL3-hyCsSxyZ",
        "outputId": "9378b039-4a98-42f1-f597-a2c068ddcd9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/KC4.0_MultilingualNMT/vi_lo_bpe.model\n",
            "/content/KC4.0_MultilingualNMT/vi_lo_bpe.vocab\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /content/vi_lo_bpe.* /content/KC4.0_MultilingualNMT_retrain/"
      ],
      "metadata": {
        "id": "KQmlLatuSej9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbdbe92a-180f-4e0c-ee59-a326f65ef923"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mv: cannot stat '/content/vi_lo_bpe.*': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Thực hiện tokenize**"
      ],
      "metadata": {
        "id": "O6ehiqycTh-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /con"
      ],
      "metadata": {
        "id": "tv-IcWHFRH4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load(\"/content/KC4.0_MultilingualNMT_retrain/vi_lo_bpe.model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "hRryNrGlTebL",
        "outputId": "31294461-08de-45ed-d2ea-f8cc45038b10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "Not found: \"/content/KC4.0_MultilingualNMT_retrain/vi_lo_bpe.model\": No such file or directory Error #2",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-a622339a9bcc>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentencePieceProcessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/KC4.0_MultilingualNMT_retrain/vi_lo_bpe.model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sentencepiece/__init__.py\u001b[0m in \u001b[0;36mLoad\u001b[0;34m(self, model_file, model_proto)\u001b[0m\n\u001b[1;32m    959\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mmodel_proto\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoadFromSerializedProto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_proto\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 961\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoadFromFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    962\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sentencepiece/__init__.py\u001b[0m in \u001b[0;36mLoadFromFile\u001b[0;34m(self, arg)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mLoadFromFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_sentencepiece\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentencePieceProcessor_LoadFromFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_EncodeAsIds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menable_sampling\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_bos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_eos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memit_unk_piece\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Not found: \"/content/KC4.0_MultilingualNMT_retrain/vi_lo_bpe.model\": No such file or directory Error #2"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_file(input_path, output_path):\n",
        "    with open(input_path, 'r', encoding='utf-8') as fin, open(output_path, 'w', encoding='utf-8') as fout:\n",
        "        for line in fin:\n",
        "            tokens = sp.encode(line.strip(), out_type=str)\n",
        "            fout.write(' '.join(tokens) + '\\n')"
      ],
      "metadata": {
        "id": "MZXFt7tzTr8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train\n",
        "tokenize_file(\"/content/KC4.0_MultilingualNMT_retrain/data/iwslt_en_vi/train2023.vi\",  \"/content/KC4.0_MultilingualNMT_retrain/data/iwslt_en_vi/train_tok.vi\")\n",
        "tokenize_file(\"/content/KC4.0_MultilingualNMT_retrain/data/iwslt_en_vi/train2023.lo\",  \"/content/KC4.0_MultilingualNMT_retrain/data/iwslt_en_vi/train_tok.lo\")\n",
        "\n",
        "# Validation\n",
        "tokenize_file(\"/content/KC4.0_MultilingualNMT_retrain/data/iwslt_en_vi/dev2023.vi\", \"/content/KC4.0_MultilingualNMT_retrain/data/iwslt_en_vi/valid_tok.vi\")\n",
        "tokenize_file(\"/content/KC4.0_MultilingualNMT_retrain/data/iwslt_en_vi/dev2023.lo\", \"/content/KC4.0_MultilingualNMT_retrain/data/iwslt_en_vi/valid_tok.lo\")"
      ],
      "metadata": {
        "id": "Z3H3jQjpTwM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ghi đè lên dữ liệu cũ để bắt đầu training"
      ],
      "metadata": {
        "id": "S5yz9f4jT56R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /content/KC4.0_MultilingualNMT_retrain/data/iwslt_en_vi/train_tok.vi /content/KC4.0_MultilingualNMT_retrain/data/iwslt_en_vi/train2023.vi\n",
        "!mv /content/KC4.0_MultilingualNMT_retrain/data/iwslt_en_vi/train_tok.lo /content/KC4.0_MultilingualNMT_retrain/data/iwslt_en_vi/train2023.lo\n",
        "\n",
        "!mv /content/KC4.0_MultilingualNMT_retrain/data/iwslt_en_vi/valid_tok.vi /content/KC4.0_MultilingualNMT_retrain/data/iwslt_en_vi/dev2023.vi\n",
        "!mv /content/KC4.0_MultilingualNMT_retrain/data/iwslt_en_vi/valid_tok.lo /content/KC4.0_MultilingualNMT_retrain/data/iwslt_en_vi/dev2023.lo"
      ],
      "metadata": {
        "id": "uZbtWUx4T-D6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training**"
      ],
      "metadata": {
        "id": "MhV2hy8ZUB5t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Đang để YaML bản cao hơn requirements nên có cái này"
      ],
      "metadata": {
        "id": "Hk5xxzUBW57e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "\n",
        "# Monkey patch: thay thế yaml.load bằng phiên bản an toàn có Loader\n",
        "def safe_yaml_load_patch():\n",
        "    orig_load = yaml.load\n",
        "    def safe_load_with_loader(*args, **kwargs):\n",
        "        if 'Loader' not in kwargs:\n",
        "            kwargs['Loader'] = yaml.SafeLoader\n",
        "        return orig_load(*args, **kwargs)\n",
        "    yaml.load = safe_load_with_loader\n",
        "\n",
        "safe_yaml_load_patch()\n"
      ],
      "metadata": {
        "id": "fxyl0B6LW5WN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh /content/KC4.0_MultilingualNMT_retrain/data/iwslt_en_vi/\n",
        "!head -n 5 /content/KC4.0_MultilingualNMT_retrain/data/iwslt_en_vi/train2023.vi\n",
        "!head -n 5 /content/KC4.0_MultilingualNMT_retrain/data/iwslt_en_vi/train2023.lo\n",
        "!head -n 5 /content/KC4.0_MultilingualNMT_retrain/data/iwslt_en_vi/dev2023.lo\n",
        "!head -n 5 /content/KC4.0_MultilingualNMT_retrain/data/iwslt_en_vi/dev2023.vi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5R7qVlvTXv9W",
        "outputId": "faabdc3c-f312-4e1d-a2a5-d7298d230fe9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 79M\n",
            "-rw-r--r-- 1 root root 505K May 15 14:15 dev2023.lo\n",
            "-rw-r--r-- 1 root root 378K May 15 14:15 dev2023.vi\n",
            "-rw-r--r-- 1 root root 310K May 15 14:11 test2023.lo\n",
            "-rw-r--r-- 1 root root 164K May 15 14:11 test2023.vi\n",
            "-rw-r--r-- 1 root root  27M May 15 14:15 train2023.lo\n",
            "-rw-r--r-- 1 root root  20M May 15 14:15 train2023.vi\n",
            "-rw-r--r-- 1 root root  13M May 15 14:11 train.en\n",
            "-rw-r--r-- 1 root root  18M May 15 14:11 train.vi\n",
            "-rw-r--r-- 1 root root 137K May 15 14:11 tst2012.en\n",
            "-rw-r--r-- 1 root root 184K May 15 14:11 tst2012.vi\n",
            "-rw-r--r-- 1 root root 130K May 15 14:11 tst2013.en\n",
            "-rw-r--r-- 1 root root 180K May 15 14:11 tst2013.vi\n",
            "▁Nếu ▁anh ▁chị ▁cảm ▁thấy ▁e ▁ngại ▁khi ▁giới ▁thiệu ▁về ▁tin ▁mừng , ▁hãy ▁tập ▁bắt ▁chuyện ▁mà ▁không ▁nhất ▁thiết ▁làm ▁chứng .\n",
            "▁H ẳn ▁nhiều ▁người ▁thấp ▁kém ▁e ▁sợ ▁và ▁tránh ▁mặt ▁những ▁nhà ▁lãnh ▁đạo ▁ấy , ▁thay ▁vì ▁xin ▁họ ▁giúp ▁đỡ ▁hoặc ▁chỉ ▁dẫn .\n",
            "▁21 ▁B ấy ▁giờ , ▁khi ▁hoàng ▁hậu ▁trông ▁thấy ▁vẻ ▁kinh ▁hãi ▁của ▁các ▁tôi ▁tớ , ▁bà ▁cũng ▁cảm ▁thấy ▁sợ ▁hãi ▁vô ▁cùng , ▁e ▁rằng ▁điều ▁dữ ▁sẽ ▁đến ▁với ▁bà .\n",
            "▁Q uyết ▁định ▁để ▁thay ▁đổi ▁thuộc ▁vào ▁chính ▁các ▁anh ▁chị ▁em , ▁và ▁chỉ ▁một ▁mình ▁các ▁anh ▁chị ▁em ▁mà ▁thôi .\n",
            "▁Những ▁lựa ▁chọn ▁của ▁các ▁em ▁ngay ▁trước ▁mắt ▁là ▁quan ▁trọng ▁vĩnh ▁viễn .\n",
            "▁ຖ້າ ▁ເຈົ້າ ▁ຮູ້ສຶກ ▁ຢ້ານ ▁ທີ່ ▁ຈະ ▁ປະກາດ ▁ຂໍ ▁ໃຫ້ ▁ຊ ້ອມ ▁ເວົ້າ ▁ໃນ ▁ແບບ ▁ທີ່ ▁ຍັງ ▁ບໍ່ ▁ມີ ▁ເປົ້າ ▁ຫມາຍ ▁ປະກາດ .\n",
            "▁ຄົນ ▁ຕໍ່າ ▁ຕ ້ອຍ ▁ຈໍານວນ ▁ຫຼາຍ ▁ຄົງ ▁ຈະ ▁ຫຼ ົບ ▁ຫຼີກ ▁ຜູ້ ▁ນໍາ ▁ດັ່ງ ▁ກ່າວ ▁ແທນ ▁ທີ່ ▁ຈະ ▁ຂໍ ▁ຄວາມ ▁ຊ່ວຍເຫຼືອ ▁ຫຼື ▁ການ ▁ຊີ້ ▁ນໍາ ▁ຈາກ ▁ເຂົາ ▁ເຈົ້າ .\n",
            "▁21 ▁ບັດ ▁ນີ້ ເມື່ອ ລາ ຊິນ ີ ▁ເຫັນ ▁ຄວາມ ▁ຢ້ານ ▁ກົວ ▁ຂອງ ▁ພວກ ▁ຂ້າ ▁ໃຊ້ ▁ນາງ ▁ກໍ ▁ເລີ່ມ ▁ມີ ▁ຄວາມ ▁ຢ້ານ ▁ກົວ ▁ຫລາຍ ▁ຂຶ້ນ , ▁ຢ້ານ ▁ວ່າ ▁ສິ່ງ ▁ບໍ່ ▁ດີ ▁ຈະ ▁ເກີດ ▁ກັບ ▁ນາງ .\n",
            "▁ການ ▁ຕັດສິນ ▁ໃຈ ▁ທີ່ ▁ຈະ ▁ປ່ຽນ ແປງ ▁ກໍ ▁ເປັນ ▁ຂອງ ▁ທ່ານ ▁, ▁ແລະ ▁ເປັນ ▁ຂອງ ▁ທ່ານ ▁ຄົນ ▁ດຽວ .\n",
            "▁ການ ▁ເລືອກ ▁ທີ່ ▁ພວກ ▁ເຈົ້າ ▁ເຮັດ ▁ໃນ ▁ເວລາ ▁ນີ້ ▁ມີ ▁ຄວາມ ▁ສໍາຄັນ ▁ຕະຫລອດ ▁ການ .\n",
            "▁ໃນ ການປ າກ ເວົ້າ ຍາວ ▁3 ▁ນາທີ ຂອງນາງ , ▁ດ ຣ ▁Lau ra ▁Tr ice ▁ສະ ▁ເຫນີ ▁ຄວາມຄິດ ກ່ຽວກັບ ພ ະລັງ ຂອງ ▁2 ▁ຊົ່ວໂມງ ▁& ▁la qu o ; ▁ຂໍຂອບໃຈ ▁& ▁la qu o ; ▁- ▁ເຮັດໃຫ້ ມິດ ຕະພາບ ຂອງທ່ານ ເລິກ ເຊິ່ງ , ▁ເພີ່ມ ທະວີ ຄວາມເປັນ ມິດ , ▁ແລະ ໃຫ້ ແນ່ໃຈວ່າ ຄົນອື່ນ ຮູ້ ວ່າມັນ ມີຄວາມ ▁ຫມາຍ ▁ແນວ ໃດ ຕໍ່ ທ່ານ . ▁ຈົ່ງ ຍົກ ເລ ີກ .\n",
            "▁ບາງທີ ມັນ ກໍ່ ລົ້ມ ເຫລ ວ . ▁ແລະຫຼັງຈາກນັ້ນ , ▁ຄວາມ ຮັບຜິດຊອບ ຂອງພວກເຮົາແມ່ນ ຫຍັງ ?\n",
            "▁ໂຄງການ ດັ່ງກ່າວ ▁ກໍ ມ ຸ່ງ ໄປ ເຖິງ ການ ສ້າງ ສະພາບແວດລ້ອມ ນະໂຍບາຍ ທີ່ມີ ຜົນປະໂຫຍດ ໃຫ້ແກ່ ການ ຊຸກຍູ້ ການ ກະທໍາ ທີ່ມີຄວາມ ຮັບຜິດຊອບ ▁ແລະ ▁ເພີ່ມ ກາ ລະ ໂອກາດ ສົນທະນາ ລະຫວ່າງ ບັນດາ ຝ່າຍ ທີ່ກ່ຽວຂ້ອງ .\n",
            "▁ພ້ອມ ທັງ , ▁ບັນດາ ຄວາມ ຫຍຸ້ງຍາກ , ▁ສິ່ງ ທ້າທາຍ ທີ່ ▁ອາຊຽນ ▁ຕ້ອງ ປະເຊີນ ຫນ້າ ▁ແລະ ▁ຄວນ ສົມທົບ ກັນ ເມື່ອ ຜັນຂະຫຍາຍ ການ ສ້າງຕັ້ງ ຂອບເຂດ ໃຫ້ແກ່ ເຄືອຂ່າຍ ▁5 G ▁ກໍ່ ໄດ້ ຍົກ ອອກມາ ປຶກສາ ຫາລື .\n",
            "▁ກ່ອນ ອື່ນ ▁ຫມົດ , ▁ມັນເປັນ ຄວາມຈິງ ທີ່ວ່າ ຂ້ອຍ ບໍ່ແມ່ນ ຄົນອື່ນ , ▁ຄົນ ທີ່ ລ ້າ ສະໄຫມ ນີ້ , ▁ຕະ ຫຼາດ ເສລີ ທີ່ມີ ການຄວບຄຸມ ຂອງລັດຖະບານ .\n",
            "▁Trong ▁bài ▁nói ▁dài ▁3 ▁phút ▁, ▁Tiến ▁sĩ ▁Lau ra ▁Tr ice ▁trình ▁bày ▁suy ▁nghĩ ▁về ▁sức ▁mạnh ▁của ▁2 ▁tiếng ▁& quot ; ▁cám ▁ơn ▁& quot ; ▁-- ▁làm ▁sâu ▁sắc ▁thêm ▁tình ▁bạn ▁bè ▁, ▁th ắt ▁chặt ▁tình ▁thân ▁, ▁và ▁để ▁chắc ▁chắn ▁rằng ▁người ▁khác ▁biết ▁họ ▁có ▁ý ▁nghĩa ▁như ▁thế ▁nào ▁với ▁bạn ▁. ▁Hãy ▁thử ▁.\n",
            "▁Có ▁thể ▁sự ▁việc ▁bất ▁thành ▁. ▁Và ▁rồi ▁, ▁trách ▁nhiệm ▁của ▁chúng ▁ta ▁là ▁gì ▁?\n",
            "▁Dự ▁án ▁cũng ▁hướng ▁tới ▁tạo ▁ra ▁môi ▁trường ▁chính ▁sách ▁có ▁lợi ▁cho ▁việc ▁thúc ▁đẩy ▁hành ▁vi ▁có ▁trách ▁nhiệm ▁và ▁tăng ▁cơ ▁hội ▁đối ▁thoại ▁giữa ▁các ▁bên ▁liên ▁quan .\n",
            "▁Đồng ▁thời , ▁những ▁khó ▁khăn , ▁thách ▁thức ▁của ▁ASEAN ▁sẽ ▁phải ▁đối ▁mặt ▁và ▁cần ▁phối ▁hợp ▁khi ▁triển ▁khai ▁xây ▁dựng ▁hệ ▁sinh ▁thái ▁cho ▁mạng ▁5 G ▁cũng ▁được ▁đưa ▁ra ▁thảo ▁luận .\n",
            "▁Trước ▁tiên ▁, ▁đúng ▁là ▁tôi ▁cũng ▁chẳng ▁phải ▁là ▁ai ▁đó ▁, ▁con ▁người ▁lỗi ▁thời ▁này ▁, ▁thị ▁trường ▁tự ▁do ▁với ▁kiểm ▁soát ▁chính ▁phủ ▁.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/KC4.0_MultilingualNMT_retrain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uz2ZUiF_TIn6",
        "outputId": "e2d511af-fbb6-42f5-bfd4-17cc8cdd907c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/KC4.0_MultilingualNMT_retrain\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "!mkdir -p models/vi_lo.model\n",
        "\n",
        "!python -m bin.main train --model Transformer --model_dir ./models/lo-vi.mamba.model --config config/bilingual_prototype.yml\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wfz-abtqUFwA",
        "outputId": "93e80227-8628-4496-f31b-0f9b54dfa891"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<frozen runpy>:128: RuntimeWarning: 'bin.main' found in sys.modules after import of package 'bin', but prior to execution of 'bin.main'; this may result in unpredictable behaviour\n",
            "Config specified, copying all to model dir\n",
            "Loaded path is a list of locations. Load in the order received, overriding and merging as needed.\n",
            "Building vocab from received data.\n",
            "No checkpoint found, start from beginning.\n",
            "[Thu, 15 May 2025 14:15:29 INFO] .lo * src vocab size = 8178\n",
            "[Thu, 15 May 2025 14:15:29 INFO] .vi * tgt vocab size = 6146\n",
            "[Thu, 15 May 2025 14:15:29 INFO] Building model...\n",
            "Zero checkpoint detected, reinitialize the model\n",
            "[Thu, 15 May 2025 14:15:34 INFO] Transformer(\n",
            "  (encoder): Encoder(\n",
            "    (embed): Embedding(8178, 512)\n",
            "    (pe): PositionalEncoder(\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (layers): ModuleList(\n",
            "      (0-5): 6 x EncoderLayer(\n",
            "        (norm_1): Norm()\n",
            "        (norm_2): Norm()\n",
            "        (attn): MultiHeadAttention(\n",
            "          (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (ff): FeedForward(\n",
            "          (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        )\n",
            "        (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "        (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (norm): Norm()\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (embed): Embedding(6146, 512)\n",
            "    (pe): PositionalEncoder(\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (layers): ModuleList(\n",
            "      (0-5): 6 x DecoderLayer(\n",
            "        (norm_1): Norm()\n",
            "        (norm_2): Norm()\n",
            "        (norm_3): Norm()\n",
            "        (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "        (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        (dropout_3): Dropout(p=0.1, inplace=False)\n",
            "        (attn_1): MultiHeadAttention(\n",
            "          (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (attn_2): MultiHeadAttention(\n",
            "          (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (ff): FeedForward(\n",
            "          (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (norm): Norm()\n",
            "  )\n",
            "  (out): Linear(in_features=512, out_features=6146, bias=True)\n",
            ")\n",
            "[Thu, 15 May 2025 14:15:34 INFO] Encoder: 23102464\n",
            "[Thu, 15 May 2025 14:15:34 INFO] Decoder: 28371968\n",
            "[Thu, 15 May 2025 14:15:34 INFO] * Number of parameters: 51474432\n",
            "[Thu, 15 May 2025 14:15:34 INFO] Starting training on cuda\n",
            "[Thu, 15 May 2025 14:16:35 INFO] epoch: 000 - iter: 00200 - train loss: 3.3210 - time elapsed/per batch: 60.5921 0.3030\n",
            "[Thu, 15 May 2025 14:17:35 INFO] epoch: 000 - iter: 00400 - train loss: 3.1935 - time elapsed/per batch: 60.6683 0.3033\n",
            "[Thu, 15 May 2025 14:18:39 INFO] epoch: 000 - iter: 00600 - train loss: 2.9379 - time elapsed/per batch: 63.1269 0.3156\n",
            "[Thu, 15 May 2025 14:19:44 INFO] epoch: 000 - iter: 00800 - train loss: 2.8370 - time elapsed/per batch: 65.1338 0.3257\n",
            "[Thu, 15 May 2025 14:20:48 INFO] epoch: 000 - iter: 01000 - train loss: 2.7916 - time elapsed/per batch: 64.6847 0.3234\n",
            "[Thu, 15 May 2025 14:21:53 INFO] epoch: 000 - iter: 01200 - train loss: 2.7254 - time elapsed/per batch: 65.0308 0.3252\n",
            "[Thu, 15 May 2025 14:23:41 INFO] epoch: 000 - iter: 01334 - valid loss: 6.1026 - bleu score: 0.0000 - full evaluation time: 63.7861\n",
            "[Thu, 15 May 2025 14:24:46 INFO] epoch: 001 - iter: 00200 - train loss: 2.5982 - time elapsed/per batch: 64.4551 0.3223\n",
            "[Thu, 15 May 2025 14:25:50 INFO] epoch: 001 - iter: 00400 - train loss: 2.5364 - time elapsed/per batch: 64.0760 0.3204\n",
            "[Thu, 15 May 2025 14:26:54 INFO] epoch: 001 - iter: 00600 - train loss: 2.4822 - time elapsed/per batch: 64.1868 0.3209\n",
            "[Thu, 15 May 2025 14:27:59 INFO] epoch: 001 - iter: 00800 - train loss: 2.3947 - time elapsed/per batch: 64.7221 0.3236\n",
            "[Thu, 15 May 2025 14:29:04 INFO] epoch: 001 - iter: 01000 - train loss: 2.3234 - time elapsed/per batch: 65.2644 0.3263\n",
            "[Thu, 15 May 2025 14:30:09 INFO] epoch: 001 - iter: 01200 - train loss: 2.2833 - time elapsed/per batch: 64.7018 0.3235\n",
            "[Thu, 15 May 2025 14:35:30 INFO] epoch: 001 - iter: 01334 - valid loss: 3.2490 - bleu score: 0.2795 - full evaluation time: 277.9353\n",
            "[Thu, 15 May 2025 14:36:36 INFO] epoch: 002 - iter: 00200 - train loss: 2.1870 - time elapsed/per batch: 65.4262 0.3271\n",
            "[Thu, 15 May 2025 14:37:40 INFO] epoch: 002 - iter: 00400 - train loss: 2.1522 - time elapsed/per batch: 64.5459 0.3227\n",
            "[Thu, 15 May 2025 14:38:45 INFO] epoch: 002 - iter: 00600 - train loss: 2.1138 - time elapsed/per batch: 64.8916 0.3245\n",
            "[Thu, 15 May 2025 14:39:50 INFO] epoch: 002 - iter: 00800 - train loss: 2.0810 - time elapsed/per batch: 64.9627 0.3248\n",
            "[Thu, 15 May 2025 14:40:55 INFO] epoch: 002 - iter: 01000 - train loss: 2.0393 - time elapsed/per batch: 65.0523 0.3253\n",
            "[Thu, 15 May 2025 14:42:00 INFO] epoch: 002 - iter: 01200 - train loss: 2.0048 - time elapsed/per batch: 64.7795 0.3239\n",
            "[Thu, 15 May 2025 14:43:42 INFO] epoch: 002 - iter: 01334 - valid loss: 3.8590 - bleu score: 0.0066 - full evaluation time: 58.3072\n",
            "[Thu, 15 May 2025 14:44:47 INFO] epoch: 003 - iter: 00200 - train loss: 1.9201 - time elapsed/per batch: 65.3107 0.3266\n",
            "[Thu, 15 May 2025 14:45:52 INFO] epoch: 003 - iter: 00400 - train loss: 1.9060 - time elapsed/per batch: 64.4747 0.3224\n",
            "[Thu, 15 May 2025 14:46:57 INFO] epoch: 003 - iter: 00600 - train loss: 1.8725 - time elapsed/per batch: 64.9735 0.3249\n",
            "[Thu, 15 May 2025 14:48:01 INFO] epoch: 003 - iter: 00800 - train loss: 1.8512 - time elapsed/per batch: 64.1735 0.3209\n",
            "[Thu, 15 May 2025 14:49:06 INFO] epoch: 003 - iter: 01000 - train loss: 1.8115 - time elapsed/per batch: 64.7978 0.3240\n",
            "[Thu, 15 May 2025 14:50:11 INFO] epoch: 003 - iter: 01200 - train loss: 1.7929 - time elapsed/per batch: 65.2275 0.3261\n",
            "[Thu, 15 May 2025 15:10:51 INFO] epoch: 003 - iter: 01334 - valid loss: 2.9430 - bleu score: 0.1179 - full evaluation time: 1196.5277\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W8_TtZEoJ2EM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "xong thì lưu vào"
      ],
      "metadata": {
        "id": "QbPS1ZOUJ_h0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp models/lo-vi.mamba.model/best_model_0.pkl /content/drive/MyDrive/"
      ],
      "metadata": {
        "id": "vUTPPa1VdjLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/KC4.0_MultilingualNMT_retrain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSjieuUkjJjg",
        "outputId": "255d97b3-2e33-44be-9275-28ada42178d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/KC4.0_MultilingualNMT_retrain\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m bin.main infer --model Transformer --model_dir ./models/lo-vi.mamba.model --features_file data/iwslt_en_vi/test2023.lo --predictions_file /content/drive/MyDrive/translate.en2vi.vi"
      ],
      "metadata": {
        "id": "DV6a2n0KN6b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36a39cf6-4b2b-49c1-ae88-2300c9d8e586"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<frozen runpy>:128: RuntimeWarning: 'bin.main' found in sys.modules after import of package 'bin', but prior to execution of 'bin.main'; this may result in unpredictable behaviour\n",
            "Config path not specified, load the configs in model directory which is []\n",
            "Loaded path is a list of locations. Load in the order received, overriding and merging as needed.\n",
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/content/KC4.0_MultilingualNMT/bin/main.py\", line 55, in <module>\n",
            "    model = models.AvailableModels[args.model](config=config_path, model_dir=args.model_dir, mode=run_mode)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/KC4.0_MultilingualNMT/models/transformer.py\", line 42, in __init__\n",
            "    self.SRC, self.TRG = self.loader.build_field(lower=opt.get(\"lowercase\", const.DEFAULT_LOWERCASE))\n",
            "                         ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1928, in __getattr__\n",
            "    raise AttributeError(\n",
            "AttributeError: 'Transformer' object has no attribute 'loader'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/KC4.0_MultilingualNMT_retrain/third-party"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wk7WYusmiW_4",
        "outputId": "fa4f5c73-f005-4014-d17c-9ab527cc7642"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/KC4.0_MultilingualNMT_retrain/third-party\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!perl multi-bleu.perl /content/drive/MyDrive/translate.en2vi.vi < /content/KC4.0_MultilingualNMT_retrain/data/iwslt_en_vi/test2023.vi"
      ],
      "metadata": {
        "id": "QuPcLS6cO6vb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6a077b8-b507-4873-99c9-842a2367f1f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use of uninitialized value in division (/) at multi-bleu.perl line 139, <STDIN> line 1000.\n",
            "Use of uninitialized value in division (/) at multi-bleu.perl line 139, <STDIN> line 1000.\n",
            "BLEU = 0.00, 1.0/0.0/0.0/0.0 (BP=0.051, ratio=0.251, hyp_len=28319, ref_len=112727)\n",
            "It is in-advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sacrebleu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uErRmzaOTxM",
        "outputId": "d10cf543-9701-4f22-b5c4-1de8ffb31fe1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sacrebleu\n",
            "  Using cached sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
            "Collecting portalocker (from sacrebleu)\n",
            "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (2024.11.6)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (2.0.2)\n",
            "Collecting colorama (from sacrebleu)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (5.4.0)\n",
            "Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: portalocker, colorama, sacrebleu\n",
            "Successfully installed colorama-0.4.6 portalocker-3.1.1 sacrebleu-2.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenize_file"
      ],
      "metadata": {
        "id": "HvZUkcWtOqUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "import time\n",
        "import sacrebleu # Sử dụng sacrebleu cho BLEU score chuẩn hơn\n",
        "\n",
        "# Import từ các file đã tải\n",
        "#from model import Seq2SeqTransformer # Giả sử file model.py có class này\n",
        "#from utils import get_tokenizer, load_vocab # Giả sử utils.py có các hàm này\n",
        "\n",
        "# --- CÁC THAM SỐ CẦN ĐIỀU CHỈNH ---\n",
        "MODEL_PATH = \"/content/best_model_0.pkl\"\n",
        "LAO_TEST_PATH = \"/content/test_lo.txt\"\n",
        "VIET_TEST_PATH = \"/content/test_vi.txt\"\n",
        "\n",
        "# !! QUAN TRỌNG: Đường dẫn đến file training gốc để xây dựng vocabulary !!\n",
        "# Nếu bạn không có file training, bạn không thể xây dựng vocabulary chính xác.\n",
        "# Hãy thay thế bằng đường dẫn thực tế của bạn.\n",
        "LAO_TRAIN_PATH_FOR_VOCAB = \"/content/KC4.0_MultilingualNMT_retrain/data/iwslt_en_vi/train2023.lo\" # THAY THẾ BẰNG PATH THỰC TẾ\n",
        "VIET_TRAIN_PATH_FOR_VOCAB = \"/content/KC4.0_MultilingualNMT_retrain/data/iwslt_en_vi/train2023.vi\" # THAY THẾ BẰNG PATH THỰC TẾ\n",
        "\n",
        "# Ngôn ngữ\n",
        "SRC_LANGUAGE = 'lo'\n",
        "TGT_LANGUAGE = 'vi'\n",
        "\n",
        "# Các tham số mô hình (phải giống như lúc huấn luyện)\n",
        "# Kiểm tra file train.py trong repo để lấy các giá trị này\n",
        "N_ENCODER_LAYERS = 3\n",
        "N_DECODER_LAYERS = 3\n",
        "EMBED_SIZE = 512\n",
        "N_HEAD = 8\n",
        "FFN_HID_DIM = 512\n",
        "DROPOUT_PROB = 0.1\n",
        "\n",
        "# Special symbols\n",
        "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
        "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
        "\n",
        "# --- KẾT THÚC CÁC THAM SỐ CẦN ĐIỀU CHỈNH ---\n",
        "\n",
        "# Thiết lập device\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# 1. Tokenizers\n",
        "token_transform = {}\n",
        "token_transform[SRC_LANGUAGE] = tokenize_file(\"/content/test_lo.txt\", \"/content/test_lo.txt\") # 'lo' sẽ dùng str.split\n",
        "token_transform[TGT_LANGUAGE] = tokenize_file(\"/content/test_vi.txt\", \"/content/test_vi.txt\") # 'vi' sẽ dùng pyvi\n",
        "\n",
        "# 2. Vocabularies\n",
        "# Cần file training để build vocab\n",
        "print(\"Building vocabularies from training data...\")\n",
        "try:\n",
        "    # Kiểm tra xem file train có tồn tại không\n",
        "    with open(LAO_TRAIN_PATH_FOR_VOCAB, 'r', encoding='utf-8') as f_check_lo, \\\n",
        "         open(VIET_TRAIN_PATH_FOR_VOCAB, 'r', encoding='utf-8') as f_check_vi:\n",
        "        pass # File tồn tại\n",
        "\n",
        "    vocab_transform = {}\n",
        "    vocab_transform[\"/content/test_lo.txt\"] = load_vocab(LAO_TRAIN_PATH_FOR_VOCAB, token_transform[SRC_LANGUAGE], SRC_LANGUAGE)\n",
        "    vocab_transform[TGT_LANGUAGE] = load_vocab(VIET_TRAIN_PATH_FOR_VOCAB, token_transform[TGT_LANGUAGE], TGT_LANGUAGE)\n",
        "    print(\"Vocabularies built successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"ERROR: Training files for vocabulary building not found.\")\n",
        "    print(f\"Please provide correct paths for LAO_TRAIN_PATH_FOR_VOCAB ('{LAO_TRAIN_PATH_FOR_VOCAB}') and VIET_TRAIN_PATH_FOR_VOCAB ('{VIET_TRAIN_PATH_FOR_VOCAB}').\")\n",
        "    print(\"Cannot proceed without vocabularies.\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
        "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
        "\n",
        "print(f\"Source Vocab Size: {SRC_VOCAB_SIZE}\")\n",
        "print(f\"Target Vocab Size: {TGT_VOCAB_SIZE}\")\n",
        "\n",
        "\n",
        "# 3. Khởi tạo mô hình\n",
        "transformer = Seq2SeqTransformer(N_ENCODER_LAYERS, N_DECODER_LAYERS, EMBED_SIZE,\n",
        "                                 N_HEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM, DROPOUT_PROB)\n",
        "\n",
        "print(\"Loading model weights...\")\n",
        "try:\n",
        "    # Quan trọng: map_location=DEVICE để tải lên CPU nếu không có GPU hoặc GPU khác lúc lưu\n",
        "    transformer.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
        "    transformer = transformer.to(DEVICE)\n",
        "    transformer.eval() # Đặt mô hình ở chế độ evaluation\n",
        "    print(\"Model weights loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"ERROR: Model file not found at {MODEL_PATH}\")\n",
        "    exit()\n",
        "except Exception as e:\n",
        "    print(f\"ERROR: Could not load model weights: {e}\")\n",
        "    print(\"Ensure the model architecture parameters (N_ENCODER_LAYERS, EMBED_SIZE, etc.) match the saved model.\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "# 4. Hàm trợ giúp cho quá trình transform text\n",
        "def sequential_transforms(*transforms):\n",
        "    def func(txt_input):\n",
        "        for transform in transforms:\n",
        "            txt_input = transform(txt_input)\n",
        "        return txt_input\n",
        "    return func\n",
        "\n",
        "def tensor_transform(token_ids):\n",
        "    return torch.cat((torch.tensor([BOS_IDX]),\n",
        "                      torch.tensor(token_ids),\n",
        "                      torch.tensor([EOS_IDX])))\n",
        "\n",
        "text_transform = {}\n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "    text_transform[ln] = sequential_transforms(token_transform[ln], # Tokenize\n",
        "                                               vocab_transform[ln], # Numericalize\n",
        "                                               tensor_transform)    # Add BOS/EOS and create tensor\n",
        "\n",
        "# 5. Hàm Greedy Decode và Translate (điều chỉnh từ train.py trong repo)\n",
        "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
        "    src = src.to(DEVICE)\n",
        "    src_mask = src_mask.to(DEVICE)\n",
        "\n",
        "    memory = model.encode(src, src_mask)\n",
        "    memory = memory.to(DEVICE) # Ensure memory is on the correct device\n",
        "\n",
        "    # ys bắt đầu với start_symbol, có batch size là 1\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
        "    for i in range(max_len - 1):\n",
        "        tgt_mask = (torch.triu(torch.ones((ys.size(1), ys.size(1)), device=DEVICE)) == 1).transpose(0, 1)\n",
        "        tgt_mask = tgt_mask.float().masked_fill(tgt_mask == 0, float('-inf')).masked_fill(tgt_mask == 1, float(0.0))\n",
        "\n",
        "        out = model.decode(ys, memory, tgt_mask)\n",
        "        out = out.transpose(0, 1) # Cần transpose nếu decode trả về (seq_len, batch, vocab_size)\n",
        "        prob = model.generator(out[:, -1]) # Lấy token cuối cùng của chuỗi output\n",
        "        _, next_word = torch.max(prob, dim=1)\n",
        "        next_word = next_word.item()\n",
        "\n",
        "        ys = torch.cat([ys, torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
        "        if next_word == EOS_IDX:\n",
        "            break\n",
        "    return ys\n",
        "\n",
        "def translate_sentence(model, src_sentence: str):\n",
        "    model.eval()\n",
        "    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1) # (seq_len, batch_size=1)\n",
        "    num_tokens = src.shape[0]\n",
        "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool).to(DEVICE) # No mask for encoder\n",
        "\n",
        "    # Dịch với greedy decode\n",
        "    # max_len có thể là len(src) + một chút, hoặc một giá trị cố định\n",
        "    tgt_tokens = greedy_decode(model, src, src_mask, max_len=num_tokens + 20, start_symbol=BOS_IDX).flatten()\n",
        "\n",
        "    # Chuyển từ ID về text, loại bỏ special tokens\n",
        "    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\").strip()\n",
        "\n",
        "\n",
        "# 6. Đọc dữ liệu test\n",
        "print(\"Loading test data...\")\n",
        "try:\n",
        "    with open(LAO_TEST_PATH, 'r', encoding='utf-8') as f:\n",
        "        lao_test_sentences = [line.strip() for line in f if line.strip()]\n",
        "    with open(VIET_TEST_PATH, 'r', encoding='utf-8') as f:\n",
        "        viet_test_references = [line.strip() for line in f if line.strip()]\n",
        "    print(f\"Loaded {len(lao_test_sentences)} Lao sentences and {len(viet_test_references)} Vietnamese references.\")\n",
        "    if len(lao_test_sentences) != len(viet_test_references):\n",
        "        print(\"Warning: Number of source and target test sentences do not match!\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"ERROR: Test files not found. Check LAO_TEST_PATH ('{LAO_TEST_PATH}') and VIET_TEST_PATH ('{VIET_TEST_PATH}').\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "# 7. Thực hiện Inference và đo lường\n",
        "hypotheses = []\n",
        "references_for_sacrebleu = [viet_test_references] # sacrebleu cần list of lists of refs\n",
        "\n",
        "total_inference_time = 0\n",
        "num_sentences = len(lao_test_sentences)\n",
        "\n",
        "print(f\"\\nStarting inference on {num_sentences} sentences...\")\n",
        "\n",
        "# Warm-up run (quan trọng để có phép đo thời gian chính xác hơn, đặc biệt với GPU)\n",
        "if num_sentences > 0 and DEVICE.type == 'cuda':\n",
        "    print(\"Performing warm-up run for GPU...\")\n",
        "    _ = translate_sentence(transformer, lao_test_sentences[0])\n",
        "    torch.cuda.synchronize() # Đảm bảo warm-up hoàn tất\n",
        "\n",
        "for i, lao_sent in enumerate(lao_test_sentences):\n",
        "    if (i + 1) % 100 == 0:\n",
        "        print(f\"Translating sentence {i+1}/{num_sentences}...\")\n",
        "\n",
        "    start_time = time.perf_counter()\n",
        "    translated_viet_sent = translate_sentence(transformer, lao_sent)\n",
        "\n",
        "    if DEVICE.type == 'cuda':\n",
        "        torch.cuda.synchronize() # Đồng bộ hóa để đo thời gian chính xác trên GPU\n",
        "\n",
        "    end_time = time.perf_counter()\n",
        "\n",
        "    total_inference_time += (end_time - start_time)\n",
        "\n",
        "    hypotheses.append(translated_viet_sent)\n",
        "\n",
        "    # In một vài ví dụ\n",
        "    if i < 5:\n",
        "        print(f\"  Lao (Src): {lao_sent}\")\n",
        "        print(f\"  Viet (Ref): {viet_test_references[i]}\")\n",
        "        print(f\"  Viet (Hyp): {translated_viet_sent}\\n\")\n",
        "\n",
        "# 8. Tính BLEU score\n",
        "if hypotheses and references_for_sacrebleu[0]:\n",
        "    bleu = sacrebleu.corpus_bleu(hypotheses, references_for_sacrebleu)\n",
        "    # `references_for_sacrebleu` là [[ref1_sent1, ref2_sent1,...], [ref1_sent2, ref2_sent2,...]]\n",
        "    # Ở đây ta chỉ có 1 reference cho mỗi câu nên là [[ref_sent1, ref_sent2,...]]\n",
        "    # sacrebleu.corpus_bleu(sys_stream, [ref_stream_1, ref_stream_2, ...])\n",
        "    # sys_stream: list of sentences (hypotheses)\n",
        "    # ref_stream_1: list of sentences (references)\n",
        "\n",
        "    print(\"\\n--- Evaluation Results ---\")\n",
        "    print(f\"BLEU Score: {bleu.score:.2f}\")\n",
        "    # print(f\"BLEU Score (full): {bleu}\") # In chi tiết\n",
        "else:\n",
        "    print(\"\\n--- Evaluation Results ---\")\n",
        "    print(\"BLEU Score: N/A (No hypotheses or references to compare)\")\n",
        "\n",
        "\n",
        "# 9. Tính tốc độ inference\n",
        "if num_sentences > 0:\n",
        "    avg_time_per_sentence = total_inference_time / num_sentences\n",
        "    sentences_per_second = num_sentences / total_inference_time\n",
        "    print(f\"Total inference time: {total_inference_time:.4f} seconds\")\n",
        "    print(f\"Number of sentences: {num_sentences}\")\n",
        "    print(f\"Average time per sentence: {avg_time_per_sentence:.4f} seconds\")\n",
        "    print(f\"Sentences per second: {sentences_per_second:.2f} sent/sec\")\n",
        "else:\n",
        "    print(\"No sentences processed for speed calculation.\")\n",
        "\n",
        "print(\"\\nDone.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "p012XoiRIpIN",
        "outputId": "b18c6e40-11aa-412b-b4ad-a83dab0a6bd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Building vocabularies from training data...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'load_vocab' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-580dbdac6fcb>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mvocab_transform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0mvocab_transform\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSRC_LANGUAGE\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLAO_TRAIN_PATH_FOR_VOCAB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_transform\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSRC_LANGUAGE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSRC_LANGUAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0mvocab_transform\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTGT_LANGUAGE\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVIET_TRAIN_PATH_FOR_VOCAB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_transform\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTGT_LANGUAGE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTGT_LANGUAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Vocabularies built successfully.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'load_vocab' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80BOWJFcJeaN",
        "outputId": "8d5a2a22-bc0a-442e-9177-86d6f58add92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    }
  ]
}